{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "You need to train the model and get score as big as you can\n",
    "\n",
    "Dataset: https://disk.yandex.ru/d/gcGKZ_KsXZN4VA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm.notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import urllib\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "ml_trains_data/clips/edzQilGMseY_13000.flac: truncated gzip input: File exists\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/gcGKZ_KsXZN4VA'\n",
    "final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "!wget -O data.tar.gz \"{download_url}\"\n",
    "!tar -xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu' # also you can use \"cuda\" for gpu and \"mps\" for apple silicon\n",
    "DATADIR = 'ml_trains'\n",
    "LOADER_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "#import librosa\n",
    "class Dataset(torch_data.Dataset):\n",
    "    def __init__(self, split_part: str, datadir: str, feats: nn.Module, read_labels=True):\n",
    "        super().__init__()\n",
    "        data = pd.read_csv(os.path.join(datadir, f'{split_part}.tsv'), sep='\\t')\n",
    "        if read_labels:\n",
    "            labels = {key: idx for idx, key in enumerate(sorted(set(data.label.values)))}\n",
    "            self._idx_to_label = {idx: key for idx, key in enumerate(sorted(set(data.label.values)))}\n",
    "            self._classes = len(labels)\n",
    "        self._feats = feats\n",
    "        self._ytids = []\n",
    "        self._pathes = []\n",
    "        self._labels = []\n",
    "        for _, row in data.iterrows():\n",
    "            path = os.path.join(datadir, 'clips', f'{row.YTID}.flac')\n",
    "            if os.path.exists(path):\n",
    "                self._ytids.append(row.YTID)\n",
    "                self._pathes.append(path)\n",
    "                self._labels.append(labels[row.label] if read_labels else 0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(self._pathes[index])\n",
    "            #print(waveform)\n",
    "            #waveform = librosa.filters.mel(n_fft=2048, sr=22050, n_mels=40)\n",
    "            #print(waveform)\n",
    "        except Exception as err:\n",
    "            logging.error(\"Can't read file %s\", self._pathes[index])\n",
    "            raise err\n",
    "        assert sample_rate == 16000\n",
    "        assert waveform.shape[0] == 1\n",
    "        feats = self._feats(waveform)[0]\n",
    "        return (feats, self._labels[index])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._pathes)\n",
    "\n",
    "    def classes(self) -> int:\n",
    "        return self._classes\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_length = max(item[0].shape[1] for item in batch)\n",
    "    X = torch.zeros((len(batch), batch[0][0].shape[0], max_length))\n",
    "    for idx, item in enumerate(batch):\n",
    "        X[idx, :, :item[0].shape[1]] = item[0]\n",
    "    targets = torch.tensor([item[1] for item in batch], dtype=torch.long).reshape(len(batch), 1)\n",
    "    return (X, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feats part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use LogMelSpectrogram or MFCC to achive better score\n",
    "class LogMelSpectrogram(nn.Module):\n",
    "    pass\n",
    "\n",
    "# FBANK 40 by default, but you can choose something else\n",
    "FEATS = 40\n",
    "transform = torchaudio.transforms.MelSpectrogram(n_mels=FEATS)\n",
    "trainset = Dataset('train', 'ml_trains', transform)\n",
    "testset = Dataset('eval', 'ml_trains', transform, read_labels=False)\n",
    "N_CLASSES = trainset.classes()\n",
    "assert N_CLASSES == 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples loaded: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total test examples loaded: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval part\n",
    "\n",
    "Write balanced accuracy:\n",
    "$$BAcc = \\frac{1}{classes}\\sum_{c = 1}^{classes} \\frac{\\sum_i^n I(y_i = p_i = c)}{\\sum_i^n I(y_i = c)}$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ -- target class for $i$ element\n",
    "- $p_i$ -- predicted class for $i$ element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of pairs (target_class, predicted_class)\n",
    "from collections import defaultdict\n",
    "def balanced_accuracy(items: list[tuple[int, int]], classes=N_CLASSES) -> float:\n",
    "    true_counts = defaultdict(int)  # Счётчик истинных классов\n",
    "    correct_counts = defaultdict(int)  # Счётчик верных предсказаний\n",
    "    \n",
    "    # Подсчёт по всем элементам\n",
    "    for y_true, y_pred in items:\n",
    "        true_counts[y_true] += 1\n",
    "        if y_true == y_pred:\n",
    "            correct_counts[y_true] += 1\n",
    "    \n",
    "    # Вычисление сбалансированной точности\n",
    "    class_accuracies = []\n",
    "    for c in range(classes):\n",
    "        if true_counts[c] > 0:\n",
    "            class_accuracies.append(correct_counts[c] / true_counts[c])\n",
    "        else:\n",
    "            class_accuracies.append(0.0)  # Если класс отсутствует\n",
    "    \n",
    "    return sum(class_accuracies) / classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 1)], 2), 1.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 1), (1, 0)], 2), 0.0)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (0, 0), (1, 0)], 2), 0.5)\n",
    "assert np.isclose(balanced_accuracy([(0, 0), (1, 1), (0, 0), (0, 0), (1, 0), (0, 1)], 2), 0.625)\n",
    "assert np.isclose(balanced_accuracy([(1, 1), (0, 1), (2, 2)], 3), 0.66666666666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part\n",
    "Train some model with as big balanced accuracy as you can\n",
    "\n",
    "You can train any model you want. The only limitation is that it must be trained from scratch on the data provided in the task. For example you can choose model from:\n",
    "- DNN\n",
    "- CNN 1d\n",
    "- CNN 2d\n",
    "- Transformer\n",
    "- RNN\n",
    "- mixes of given models\n",
    "\n",
    "Hints:\n",
    "- No need to train large models for this task. 10 million parameters is more than you need.\n",
    "- Watch to overfitting, try to add Augmentation, Dropout, BatchNorm, L1/L2-Regulatization or something else.\n",
    "- Use poolings or strides to reduce time-dimenstion. It is better to reduce the dimension gradually rather than at the end.\n",
    "- Try different features (mel-spec, log-mel-spec, mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(\n",
    "    model: nn.Module,\n",
    "    data: torch_data.Dataset,\n",
    "    opt: optim.Optimizer,\n",
    "    batch_size: int = 256,\n",
    "    train: bool = True\n",
    "):\n",
    "    loader = torch_data.DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss_sum, batches = 0.0, 0\n",
    "    pred_pairs = []\n",
    "    for X, Y in tqdm.tqdm(loader):\n",
    "        pred = model.forward(X.to(DEVICE))\n",
    "        loss = F.cross_entropy(pred.squeeze(), Y.squeeze().to(DEVICE))\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        with torch.no_grad():\n",
    "            pred_pairs.extend(zip(\n",
    "                Y.data.numpy().reshape(-1),\n",
    "                torch.argmax(pred, dim=1).cpu().data.numpy().reshape(-1)\n",
    "            ))\n",
    "    return loss_sum / batches, balanced_accuracy(pred_pairs)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    opt,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 1,\n",
    "):\n",
    "    train_data, val_data = torch.utils.data.random_split(trainset, (0.9, 0.1), generator=torch.Generator().manual_seed(42))\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = stage(model, train_data, opt, batch_size=batch_size)\n",
    "        val_loss, val_acc = stage(model, val_data, opt, batch_size=batch_size, train=False)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        clear_output()\n",
    "        #fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        #axis[0].plot(np.arange(1, epoch + 2), train_losses, label='train')\n",
    "        #axis[0].plot(np.arange(1, epoch + 2), val_losses, label='val')\n",
    "        #axis[1].plot(np.arange(1, epoch + 2), train_accs, label='train')\n",
    "        #axis[1].plot(np.arange(1, epoch + 2), val_accs, label='val')\n",
    "        #axis[0].set(xlabel='epoch', ylabel='CE Loss')\n",
    "        #axis[1].set(xlabel='epoch', ylabel='Accuracy')\n",
    "        #fig.legend()\n",
    "        #plt.show()\n",
    "        print(f'Epoch {epoch + 1}.')\n",
    "        print(f'Train loss {train_loss}. Train accuracy {train_acc}.')\n",
    "        print(f'Test loss {val_loss}. Test accuracy {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim=FEATS, out_dim=N_CLASSES):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_dim, 64, kernel_size=3, stride=1, padding=1)  # [batch_size, 64, TIME]\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1)  # [batch_size, 128, TIME/2]\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1)  # [batch_size, 256, TIME/4]\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # [batch_size, 256, 1]\n",
    "        self.dropout = nn.Dropout(0.3)  # Regularization\n",
    "        self.fc = nn.Linear(256, out_dim)  # Fully connected layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X: [batch_size, in_dim, TIME]\n",
    "        X = F.relu(self.bn1(self.conv1(X)))  # Convolution + BatchNorm + ReLU\n",
    "        X = F.relu(self.bn2(self.conv2(X)))  # Downsampling\n",
    "        X = F.relu(self.bn3(self.conv3(X)))  # Further Downsampling\n",
    "        X = self.global_pool(X).squeeze(-1)  # Global average pooling\n",
    "        X = self.dropout(X)  # Apply dropout\n",
    "        X = self.fc(X)  # Final classification layer\n",
    "        return X  # [batch_size, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589000c48ca4425595ad24043705f06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Can't read file ml_trains\\clips\\BL5sm5PRC1w_50000.flac\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ml_trains\\clips\\BL5sm5PRC1w_50000.flac and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      2\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, batch_size, epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m train_losses, val_losses, train_accs, val_accs \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 47\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m stage(model, val_data, opt, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m, in \u001b[0;36mstage\u001b[1;34m(model, data, opt, batch_size, train)\u001b[0m\n\u001b[0;32m     19\u001b[0m loss_sum, batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m pred_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(loader):\n\u001b[0;32m     22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[0;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred\u001b[38;5;241m.\u001b[39msqueeze(), Y\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(DEVICE))\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[22], line 30\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     29\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pathes[index])\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sample_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16000\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m waveform\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m         waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pathes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m#print(waveform)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m#waveform = librosa.filters.mel(n_fft=2048, sr=22050, n_mels=40)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m#print(waveform)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[1;32mc:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[1;34m(uri, format, backend_name)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri ml_trains\\clips\\BL5sm5PRC1w_50000.flac and format None."
     ]
    }
   ],
   "source": [
    "model = Model().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())\n",
    "train(model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction part\n",
    "\n",
    "Prepare result file and send it to Yandex.Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for idx, ytid in enumerate(testset._ytids):\n",
    "    feats, _ = testset[idx]\n",
    "    predict = torch.argmax(model.forward(feats.reshape(1, FEATS, -1).to(DEVICE)), dim=1).item()\n",
    "    result.append((ytid, trainset._idx_to_label[predict]))\n",
    "pd.DataFrame(result, columns=['YTID', 'label']).to_csv('result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file1 = 'result_38_5.tsv'\n",
    "file2 = 'result_38_8.tsv'\n",
    "\n",
    "df1 = pd.read_csv(file1, sep='\\t')\n",
    "df2 = pd.read_csv(file2, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1)):\n",
    "    if i % 2 == 1:  # Для нечётных индексов\n",
    "        df1.loc[i, 'label'] = df2.loc[i, 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df1['label'] = [\n",
    "    random.choice([val1, val2]) for val1, val2 in zip(df1.iloc[:, 1], df2.iloc[:, 1])\n",
    "]\n",
    "\n",
    "# Сохранение результата\n",
    "df1.to_csv('result.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
