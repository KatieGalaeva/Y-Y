{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=new_tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 98, 8944, 28498, 6738, 27852, 19370, 1046, 3287, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"► ◊◒▴ ▽◠◓◠◳◠▼◠ ▱◂▱◗▻◧▻ ◕◫◀◗▵\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katie\\AppData\\Local\\Temp\\ipykernel_20652\\1372133618.py:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"bleu\")\n",
      "c:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "import random\n",
    "import itertools\n",
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train = []\n",
    "with open('train') as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line.strip()))\n",
    "val = []\n",
    "with open('val') as f:\n",
    "    for line in f:\n",
    "        val.append(json.loads(line.strip()))\n",
    "with open('test_no_reference') as f:\n",
    "  test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Katie\\miniconda3\\envs\\mltest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dst=[]\n",
    "src=[]\n",
    "for i in range(len(train)):\n",
    "    dst.append(train[i]['dst'])\n",
    "    src.append(train[i]['src'])\n",
    "import spacy\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "from datasets import load_dataset\n",
    "train_zet = load_dataset('json', data_files='train', split=\"train\")\n",
    "valid_zet = load_dataset('json', data_files='val' , split=\"train\")\n",
    "test_zet = load_dataset('json', data_files='test_no_reference' , split=\"train\")\n",
    "train = train_zet\n",
    "valid = valid_zet\n",
    "pad_token = \"<pad>\"\n",
    "from transformers import AlbertTokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, wrapped_tokenizer, max_length, lower=True, sos_token=\"<sos>\", eos_token=\"<eos>\"):\n",
    "    en_tokens = [token for token in en_nlp.tokenizer(example[\"dst\"])][:max_length]\n",
    "    de_tokens = [token for token in wrapped_tokenizer.tokenize(example[\"src\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [str(token).lower() for token in en_tokens]\n",
    "        #de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    cls_token=\"[CLS]\"\n",
    "    sep_token=\"[SEP]\"\n",
    "    de_tokens = [cls_token] + de_tokens + [sep_token]\n",
    "    \n",
    "    return {\"dst\": en_tokens, \"src\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8bfec849014be094288c1a88fa8eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 25\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"wrapped_tokenizer\": wrapped_tokenizer,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "train_data = train_zet.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_zet.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'he',\n",
       " 'would',\n",
       " 'need',\n",
       " 'to',\n",
       " 'repeat',\n",
       " 'his',\n",
       " 'vows',\n",
       " 'in',\n",
       " 'the',\n",
       " 'land',\n",
       " 'of',\n",
       " 'the',\n",
       " 'living',\n",
       " 'and',\n",
       " 'drink',\n",
       " 'from',\n",
       " 'the',\n",
       " 'wine',\n",
       " 'of',\n",
       " 'ages',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['dst'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, tokenizer, wrapped_tokenizer):\n",
    "    for i in range(len(example)):\n",
    "\n",
    "        en_ids = (tokenizer(example[\"dst\"]))\n",
    "        de_ids = wrapped_tokenizer(example[\"src\"])\n",
    "        \n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs= {\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"wrapped_tokenizer\": wrapped_tokenizer}\n",
    "train_num = train_zet.map(numericalize_example,  fn_kwargs=fn_kwargs)\n",
    "valid_num = valid_zet.map(numericalize_example,  fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_num = train_num.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_num = valid_num.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "def get_collate_fn(pad_index=1):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"]['input_ids'] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"]['input_ids'] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader\n",
    "batch_size = 4\n",
    "pad_index=0\n",
    "train_data_loader = get_data_loader(train_num, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_num, batch_size, pad_index)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
    "        # encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        )\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n",
    "        )\n",
    "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_length = encoder_outputs.shape[0]\n",
    "        # repeat decoder hidden state src_length times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # hidden = [batch size, src length, decoder hidden dim]\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src length, decoder hidden dim]\n",
    "        attention = self.v_fc(energy).squeeze(2)\n",
    "        # attention = [batch size, src length]\n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        embedding_dim,\n",
    "        encoder_hidden_dim,\n",
    "        decoder_hidden_dim,\n",
    "        dropout,\n",
    "        attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n",
    "        self.fc_out = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, src length]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch size, 1, src length]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted = [batch size, 1, encoder hidden dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, encoder hidden dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, (encoder hidden dim * 2) + embedding dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output = [seq length, batch size, decoder hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, decoder hid dim]\n",
    "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, decoder hidden dim]\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        # this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, decoder hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(wrapped_tokenizer)\n",
    "output_dim = len(tokenizer)\n",
    "encoder_embedding_dim = 4\n",
    "decoder_embedding_dim = 4\n",
    "encoder_hidden_dim = 8\n",
    "decoder_hidden_dim = 8\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    decoder_dropout,\n",
    "    attention,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(100000, 4)\n",
       "    (rnn): GRU(4, 8, bidirectional=True)\n",
       "    (fc): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn_fc): Linear(in_features=24, out_features=8, bias=True)\n",
       "      (v_fc): Linear(in_features=8, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(30000, 4)\n",
       "    (rnn): GRU(20, 8)\n",
       "    (fc_out): Linear(in_features=28, out_features=30000, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,391,736 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        \n",
    "        src = batch[\"de_ids\"].to(device)\n",
    "        \n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, 0)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(epoch_loss / len(data_loader))    \n",
    "    return epoch_loss / len(data_loader)\n",
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(data_loader):\n",
    "            \n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    print(epoch_loss / len(data_loader))\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.683570602416992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:22<12:23, 82.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.230846645355225\n",
      "\tTrain Loss:   6.684 | Train PPL: 799.168\n",
      "\tValid Loss:   5.231 | Valid PPL: 186.951\n",
      "5.139537334442139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:46<11:04, 83.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.037426639556885\n",
      "\tTrain Loss:   5.140 | Train PPL: 170.637\n",
      "\tValid Loss:   5.037 | Valid PPL: 154.073\n",
      "5.0480299644470215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:07<09:37, 82.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.954266906738281\n",
      "\tTrain Loss:   5.048 | Train PPL: 155.715\n",
      "\tValid Loss:   4.954 | Valid PPL: 141.779\n",
      "4.949221080780029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:29<08:12, 82.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.867814653396606\n",
      "\tTrain Loss:   4.949 | Train PPL: 141.065\n",
      "\tValid Loss:   4.868 | Valid PPL: 130.036\n",
      "4.871167680740356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:44<06:38, 79.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.796444248199463\n",
      "\tTrain Loss:   4.871 | Train PPL: 130.473\n",
      "\tValid Loss:   4.796 | Valid PPL: 121.079\n",
      "4.800306709289551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:44<04:51, 72.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7245210227966306\n",
      "\tTrain Loss:   4.800 | Train PPL: 121.548\n",
      "\tValid Loss:   4.725 | Valid PPL: 112.677\n",
      "4.735698781967163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [08:47<03:28, 69.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.665063785552978\n",
      "\tTrain Loss:   4.736 | Train PPL: 113.943\n",
      "\tValid Loss:   4.665 | Valid PPL: 106.172\n",
      "4.681748081207275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:48<02:13, 66.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.623884813308716\n",
      "\tTrain Loss:   4.682 | Train PPL: 107.959\n",
      "\tValid Loss:   4.624 | Valid PPL: 101.889\n",
      "4.641150390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [10:52<01:05, 65.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.593417663574218\n",
      "\tTrain Loss:   4.641 | Train PPL: 103.664\n",
      "\tValid Loss:   4.593 | Valid PPL:  98.832\n",
      "4.616597009658814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:54<00:00, 71.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.572523347854614\n",
      "\tTrain Loss:   4.617 | Train PPL: 101.149\n",
      "\tValid Loss:   4.573 | Valid PPL:  96.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut3-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katie\\AppData\\Local\\Temp\\ipykernel_20652\\3715736893.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"tut3-model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"tut3-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    wrapped_tokenizer,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ids = wrapped_tokenizer(sentence)['input_ids']\n",
    "     \n",
    "        \n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        encoder_outputs, hidden = model.encoder(tensor)\n",
    "        inputs = [1]\n",
    "        attentions = torch.zeros(max_output_length, 1, len(ids))\n",
    "        for i in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, attention = model.decoder(\n",
    "                inputs_tensor, hidden, encoder_outputs\n",
    "            )\n",
    "            attentions[i] = attention\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "        \n",
    "            if predicted_token == [1]:\n",
    "                break\n",
    "        en_tokens = tokenizer.decode(inputs)\n",
    "    return en_tokens, sentence, attentions[: len(en_tokens) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'◲▨◫ ▽▪▱◈◠ ◀◗◓ ◈▩▢◪▦▱▴▦◪▦ ◪▫▨◫▦▱◗◐▴ 12▵ ▨▴▢ ▨◠▫◬▱◠◓◠▨ ◓◪▨◂◓ ▨▪◓◠▦ ◄◗▼▨▴▱◞◧▦■ ▼◨◎◠◓▫▴◞◗ ◕◪◓◉▴▨▱◪◒▫◫◓◗▱◪▦ ◍◂◨◓◀◠▱▱ ◚◪ ◈◣◓▫▱◭ (◍◧◨◓◞◂◎▴) ◂▽▾▦▱◠◓▪▦◈◠ ◢◠▻▫◠▦ ◡◗◎ ◌◨◓◳▨ ▫◠◓◠◍▪▦◈◠▦ ◂▽◨▦◈◠▦ ◠▱◬▦◈▪▵'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = valid_zet['src'][10]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    wrapped_tokenizer,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> the, the, the, the, the, the, the, the, the<pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'◲▨◫ ▽▪▱◈◠ ◀◗◓ ◈▩▢◪▦▱▴▦◪▦ ◪▫▨◫▦▱◗◐▴ 12▵ ▨▴▢ ▨◠▫◬▱◠◓◠▨ ◓◪▨◂◓ ▨▪◓◠▦ ◄◗▼▨▴▱◞◧▦■ ▼◨◎◠◓▫▴◞◗ ◕◪◓◉▴▨▱◪◒▫◫◓◗▱◪▦ ◍◂◨◓◀◠▱▱ ◚◪ ◈◣◓▫▱◭ (◍◧◨◓◞◂◎▴) ◂▽▾▦▱◠◓▪▦◈◠ ◢◠▻▫◠▦ ◡◗◎ ◌◨◓◳▨ ▫◠◓◠◍▪▦◈◠▦ ◂▽◨▦◈◠▦ ◠▱◬▦◈▪▵'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:33<00:00, 10.73it/s]\n"
     ]
    }
   ],
   "source": [
    "trans=[]\n",
    "sente=[]\n",
    "for i in tqdm.tqdm(test):\n",
    "    translation_i, sentence_tokens_i, attention_i=translate_sentence(\n",
    "    i['src'],\n",
    "    model,\n",
    "    en_nlp,\n",
    "    wrapped_tokenizer,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")\n",
    "\n",
    "    \n",
    "    trans.append(translation_i)\n",
    "    sente.append(sentence_tokens_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk> the, the the the the the the the the the, the, the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk> the the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk> the the the the the the the the the the the, the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>the, the, the, the, the, the, the, the, the, the, the,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>,, the, the, the,,,,,,,,,,,,,,,,,,',\n",
       " '<unk> the the the the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the, the, the,,,,,,,,,<pad><pad>',\n",
       " '<unk> the the the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk> the the the the the the the the the the the the the, the, the, the<pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk> the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,<pad><pad><pad>',\n",
       " '<unk> the, the, the, the, the, the, the, the, the, the<pad><pad><pad><pad><pad><pad>',\n",
       " '<unk> the, the, the, the, the, the, the, the, the, the<pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the, the,,,,,,,,,,<pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk> the the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk> the the the the the the the the the the the the the the, the<pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the,,,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk> the, the, the, the, the, the, the, the, the, the, the<pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the, the, the,,,,,,,,,<pad><pad>',\n",
       " '<unk> the, the, the, the, the, the, the, the, the, the<pad><pad><pad><pad><pad><pad>',\n",
       " '<unk> the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,<pad>',\n",
       " '<unk>, the, the, the, the, the, the,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>, the, the, the, the, the, the,,,,,,,,,<pad><pad><pad><pad>',\n",
       " '<unk> the the the the the the the the the the the the the the the the the the, the<pad><pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the, the, the, the,,,,,,,<pad><pad>',\n",
       " '<unk> the the the the the the the the the the the the<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>, the, the, the, the, the, the, the,,,,,<pad><pad><pad><pad><pad><pad>',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>,,,,,,,,,,,,,,,,,,,,,,,,,',\n",
       " '<unk>, the, the, the, the, the, the, the, the, the, the<pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['◲▦◠▦◬▦■ ◉◗▢◕◗ ◍◗▱◎ ▽◠▽▪▦◠ ◕▴◉◗▦▼▴ ◀◗◓◉◧▨ ◎▴◞◠▸ ◠▱◈▪▨ ◚◪ ◀◨ ◎◪◞◠▸▱◠◓◬▦ ◀◠▢▪▱◠◓▪ ▻◪▨ ◈◂◞▫◉◠ ◈▴◐◫▱◈◗▵',\n",
       " '▯▴▥ ◟◧◓▨▱◨ ◀◫◓ ◈◠◈◬■ ◉◂▼◨◐◨▦ ◠▦▦◪◞◗▦◗▦ ▽◠▢◈◬◐▪ ◚◪ ◳◠▦▱▪◒▱▪▨▱◠ ▨▴▦◈◗◞◗▦▴ ◕◣▦◈▴◓◈◗◐◫■ \"◀◫◓ ◞◫◳◠▷◗ ◈◠▷◠\" ◳◠▢◠▦ ◀◗◓ ◎▴◞◠▸◈◠▦ ◞◧▦◓◠ ▨◪▦◈◫◞◗▦◪ ◠◳◓▪◎▼◬▱◬▨ ◳◠▻▪▱◈▪◐▪ ◚◪ ◗◒◫▦◈◪▦ ▨◂◚◨▱◈▾◐▾ ◫◉◫▦ ◉◗◍▫▴ ◈◠◚◠ ◠◉◬▽◂◓▵',\n",
       " '◡◠▻◧▦ ◂▫◧◎◂◀◗▱ ◍◗◓◎◠◞◬ ◠▦▱◠◒◎◠◞▪▢ ◝◓▴▹◗▫ ◈◨◓▾◎▾▦◈◠ ◞▪▦◬◓◈◠ ◀◪▨▱▴◎◪ ◞◭◓◪◞◫▦◫▦ ◨▢◠◎◠◞▪▦▪▦ ◫▦◞◠▦▱◠◓▪▦ ◗◒◗▦◫ ▨◠▽◀◪▫◎◪◞◫▦◪ ▦▴◈◪▦ ◂▱◠◀◫▱▴▼▴◐◫▦◫ ◞◇◳▱◪◈◗▵',\n",
       " \"◝▾◀◀◠ ▰◠▫◞◂▦ ◚▴ ▰▴◀◀ ▮◫◎▻◞◂▦■ ◞◠◀◠▷ ◂◳▦◠▦◠▦ ◍◂◨◓◀◠▱▱ ◎◠◉▱◠◓▪▦▪▦ ▨◠▷◓◠◎◠▦▪ ▮▴◓◕◫◧ ◆◠◓▼◗◠'▽◬ ◚◪ ○▱▴▹ ▯◂◓▴▦ ◫▱▴ ▴◒▱▴◒▫◗◐◫ ◎◠◉▫◠ ▨◬◞◠ ◞▩◓◪◈◪ ◎◠◐▱▾▻ ▴▫▫◗▵\",\n",
       " '\"○◐▱◠◈◬◐▪▦▪ ◕◣◓◎◪▱◪◓◗▦◪ ◠◞▱◠ ◗▢◫▦ ◚▴◓◎▴■\" ◈◪◈◫ ◀◠▦◠▵',\n",
       " '▭◠◀▴◓▴ ◕◇◓▴ ▫◠▦▪▨▱◠◓ ◧▱◠▽ ▽◪◓◫▦◈▴▦ ▨◠◉◠▦ ◀◗◓◈▴▦ ◍◠▢▱◠ ◒◭▻▷◪▱◫ ◧▱◈▾◐▾▦◨ ◞◇▽▱◪◈◗■ ◠▦▼◠▨ ▻◂▱◗◞ ◂▱◠◳◠ ▨◠◓◬◒◠▦ ▨◗◒◫ ◞◠▽◬◞▪▦▪ ◀▴▱◗◓▫◎▴◈◗▵',\n",
       " '◆▴◓◗▱◫◎■ ◄◠◈◓◫◈ ▫◠◓◠◍▪▦◈◠▦ ◳◠◞◠ ◈◬◒▪ ◗▱◠▦ ▴◈◫▱▴▦ ◠▦▼◠▨ ◠▽◓◬▱◬▨◉▪ ◢◠▫◠▱◠▦▱◠◓▼◠ ▨◨▫▱◠▦◠▦ 1 ◰▨◫◎ ◓▴◍▴◓◠▦◈▾◎▾▦◈◠▦ ◀◫◓ ◳◬▱ ◞◧▦◓◠ ◈◠ ◀◠◐◬◎◞◬▢▱▪▨ ◳◠▦▱◬◞◬ ◀◇▱◕◪◈◪ ◳▩▨◞▴▨ ▨◠▱◈▪▵',\n",
       " '◝◂▱◗◚◳◠▱◬ ◈◫▻▱◧◎◠▫ ▤◧◀◪◓▫◂ ▬◠▱▢◠◈◗▱▱◠ ▨◧▦▾◳▱◠ ◫▱◕◗▱◗ ◧▱◠◓◠▨ \"◝◂▱◗◚▽◠ ◪▦◪◓▸◗■ ◀◗◓▱◗▨ ◚▴ ◀◪◓◠◀◪◓▱◫▨ ◓◨▷▾ ◚◪ ◞◭▨▾▦▴▫▱◪ ◈◧▱▾ ◂▱◠◓◠▨ ◚▴ ▴▱◀◪▫▫◪ ◂▱◨◎▱◨ ◀◗◓ ◀◠▨▪◒ ◠◉▪◞▪◳▱◠ ◞◂▦▾▼◨ ◀◪▨▱◗◳◧◓■\" ◫◍◠◈◪▱▴◓◗▦◫ ▨◨▱▱◠▦◈◬▵',\n",
       " \"▮◠◗▦◞◀◨◓◳'◞ ◆▴▦▴▱ ◄◭◈◭◓▩ ◄◫▨▴ ▬◧◨▻▴■ ◞◠▫▪◒ ▦◧▨▫◠▱◠◓◬▦▪▦ ◈◠▷◠ ◉◧▨ ◕◬◈◠ ◈▪◒◬ ▷◗▢◎◪▫ ◞◨▦◠▦ ○▱◈◫ ◚◪ ▲◗◈▱ ◗▦◈◫◓◗◎▱◗ ◞◠▫◬◒ ◎◠◐◠▢◠▱◠◓▪◳▱◠ ◓◪▨◠◀◪▫ ▴▫◎▴▨ ◗◉◫▦ ◞◭▻◪◓◎◠◓▨◪▫ ▢◫▦▼◫◓◗▦◫▦ ◕◗◈◪◓◪▨ ◈◠▷◠ ◉◂▨ ◈◪▻◠◓▫◎◠▦ ◎◠◐◠▢◠▱◠◓▪▦◠ ◀▴▦▢◪▽◪▼▴◐◫▦◫ ◞◣◳▱▴◈◫▵\",\n",
       " '◆◇◞▫▴◓◗ ◀◠◒▱◠◈▪ ◠▦▼◠▨ ◍◠◓▨▱◬ ◀◫◓ ◓◂▫◠ ◫▢▱◪◎◪▨ ▢◂◓◨▦◈◠ ▨◠▱◈◬▵',\n",
       " '◰▱◚◗▴ ◕◣◐▩◞ ▻◂◎▻◠◞◬■ ◪◎▢◗◓◎▴ ◞▩▫▽▴▦◗▦◗▦ ◗◉◫▦◪ ▫◠◎ ◧▫◨◓▾◓▵ (◰▱◚◫◪/◄◂▫▷▴◓)',\n",
       " '\"◝▾▦◠ ◓◠◐◎◪▦■ ◦◝◙\\'◈◪▦ ◀▾▦▾▦ ▨◠◓◒◬▱◬◐▪▦▪ ◕◇◓▴◎◫◳◂◓◨▢■\" ◈◫◳▴ ◪▨▱▴◈◗▵',\n",
       " \"◦▱◠◞▨◠ ◰▽◠▱◪▫ □◧▱◫◞▱◪◓◫'▦◫▦ ▼◨◎◠◓▫▴◞◗ ◕▩▦▩ ▽◠▽▪▦▱◠◈▪◐▪ ◀◫◓ ◓◠▻◂◓◠ ◕◇◓◪ ▰◠◞◗▱▱◠■ ○▱◠◞▨◠▱▪ 29 ▽◠◒◬▦◈◠▨◫ □◠▱◗▦ ◠◫▱▴ ◫◉◫ ◒◗◈◈▴▫ ▾◳◕◨▱◠◎◠■ ◠◗▱◪ ◫◉◫ ◒◗◈◈▴▫◪ ◈◠◗◓ ◀◗◓ ◓◠▻◧◓◠ ◎◭◈◠▷◠▱▴ ▴▫◎◪ ◚▴ ▻◂▱◫◞▴ ◎◨▨◠◚▴◎◪▫ ◞◨◉▱◠◓◬◳▱◠ ▫▾▫▾▨▱◠▦◈▪▵\",\n",
       " '◝◓◧◧▨◫▦◕◞ ◘▦◞▫◗▫◭◞◭\\'▦◈▴▦ ◠◓▨◠◈◠◒▪◎ ◧▱◠▦ ▯◂◓◎◠▦ ◰◗◞▴▦■ ▰◠◞▷◫▦◕▫◂▦ ▮◧◓◨◎▱◨▱◨▨ ◚◪ ◰▫◫▨ ▮◠◚◨▦◨▼◨▱◠◓▪\\'▦◬▦ ◀◠◒▨◠▦◬ ◚◪ \"▶▷◪ ▲◠◞▫ □◠▱◠▼▴: ◰▾◓◧▻▴\\'◞ ▶▾◓◀◨▱▴▦▫ ▬◪▦▫◨◓◳ ◫▦ ◌◫◚◪ ▲◫◚◪◞ ◠▦◈ ◱▦◪ ▲▴◕▴▦◈◠◓▽ ▭◧▾◞▴\" (▮◧▦ ▮◠◓◠▽: ◝▴◒ ▧◎◭◓◈▴ ◚▴ ◰◍◞◠▦▴◚◫ ◝◗◓ ◰◚◈▴ ◦◚◓◨▻◠\\'▦◬▦ ◅◠▱▨◠▦▫▪▱◬ ◟▩▢▽▪▱◬) ▨◫▫◠◀▪▦◬▦ ◳◠▢◠◓▪▵',\n",
       " '▮◠▨◠▫▱▪▨▱◠◓▪◎ ◚◪ ▨▾▱▩◀▩▦ ▽◠◒◠◈▪◐▪ ◈◪◐◗◒◫▨▱◗▨▱◪◓▱▴ ◀◪◓◠◀◪◓ ◕▴◉▫◫◐◫◎◗▢ ◳◬▱ ◕◪◓◉▴▨▫◪▦ ▢◧◓ ◕◪◉▫◫ ◠▦▼◠▨ ◒▾ ◠▦▨◫ ▨◂▦◨◎◨◎◨▢◨▦ ◉◂▨ ◫◳◗ ◧▱◈◨◐◨▦▾ ◈◭◒◭▦◭▽◧◓◨◎▵',\n",
       " \"▮◨◎◠▫◓◠ ▨▪◳▪◞◬ ◠◉▪▨▱◠◓▪▦◈◠▨◗ ▯◗◠◞ ○◈◠◞▪'▦◈◠ ◳◭▢▱▴◓▼◪ ▨◫◒◫ ▷◠▽◠▫▪▦◬ ▨◠▽◀▴▫▫◗▵\",\n",
       " '\"◢◠▦▫◪\\'▦◫▦ ◕◪▱◈◗◐◫▦◗ ◕◣◓◭▻ ◀◫◓ ▨▴◓▴ ◈◂▨▾▦◈◨◎▵ ◅◧▨ ◍◠▢▱◠ ◈▩◒▩▦◎▴◈◪▦ ◒◨▫▾ ◉◪▨▫◫◎▵\"',\n",
       " '◄◠◳\\'◗▦ ◕◠▢▴▫◪▦◫▦ ◀◠◒ ◞◠▽◍◠◞▪▦◈◠▨◫ ◓◣▻◧◓▫◠▸◬▦◬▦ ▷▴◎▴▦ ▽◠▦◬▦◈◠▨◫ ◀◗◓ ◀◠◒▨◠ ◓◇▻◂◓▫◠▸◈◠■ ◪◞▨◗ ◈◬◒ ◫◒▱▴◓◫ ◀◠▨◠▦◬ ◝◂◓◫◞ ◡◂▷▦◞◂▦■ ◄◠◳\\'◫▦ ◝◓▴▹◫▫ ◗◉◗▦ ◞◇▢◈◪ \"◈◠◎◠ ◂◳◨▦◨\"▦◠ ▨◠◓◒▪ ▷◠◎▱▴◞◫▦◗■ ◝◓◫▫◠▦▽◠ ◗▱◪ ◦◝\\'▦◗▦ ◀◗◓◀◫◓◗▦◫▦ ◚▴◓◕◗◞◫▦◫ ▫◧▻▱◠◎◠▨ ◧▱◠▦ ◄◠▽\\'◫▦ ◣▦◪◓◫◞◗▦◗▦ \"▫◠◎◠◎◬◳▱◠ ◞◠◉◎◠▱▪▨\" ◂▱◈◨◐◨▦▾ ◞◣◳▱▴◳◪◓◪▨ ▨▾◚◚◪▫▱◪▦◈◗◓◈◗▵',\n",
       " '◖◗◈◈▴▫▱◪ ◠▢◠◓▱◠▦▪◓▨◪▦ ◀◠◒▪ ◇▦▩▦◈▴ ◠▽◠▨▫◠ ◀◫◓▨◠◉ ◈◠▨◫▨◠ ◀◂▽▾▦▼◠ ◈◨◓◎◠◞▪ ▬▯▯ ▨◠▦◠▱▪▦◈◠ ▼◠▦▱◬ ◂▱◠◓◠▨ ▽◠▽▪▦▱◠▦◈◬▵',\n",
       " \"◙◠▷◠ ◞◧▦◓◠ ▷▪▢▱◬ ◀◫◓ ◀◗▫◫◓◎▴◳◫ ▷◪◈◪◍▱◪◳◪◓◪▨ ◈◣◓◈◭▦▼◭ ◞◬◓◠◈◠▨◗ ◢◗◎◫ ▤◠◫▨▨◂▦◪▦'◗ ◕◪◉◎◪▨ ◗◉◫▦ ▱◠◞▫◫▨▱▴◓◫▦◗ ◳◠▨◎◠▨ ▻◠▷◠◞▪▦◠ ▽◠◓▪◒◬▦ ◀▩▽▩▨ ◀◫◓ ▨◬◞◎◬▦◠ ▱◗◈◪◓▱◗▨ ▴▫▫◫▵\",\n",
       " '◦▦◗ ◞◬▼◠▨▱◬▨ ◈▴◐◫◒◫◎▱◪◓◗▦◪■ ▻◠▢◠◓ ◕▩▦▩ ◧◓▫◠ ○◝◙ ◀◧◳▾▦▼◠ ◂▱▾◒◎◨◒ ◎▩▫▷◗◒ ◀◗◓ ◇◓▦▴▨ ◚◠◓▵',\n",
       " '▶◠◓◗▷ ◀◇◳▱◪ ◈◣▦◭◎ ▦◂▨▫◠▱◠◓▪ ◪▫◓◠◍▪▦◈◠ ◒▴▨◫▱▱▴▦◫◓ ◚◪ ◈◪◎◧▨◓◠◞◗▦◫▦ ◈▾◓◈◨◓◨▱◠◎◠▢ ◫▱◪◓▱▴▽◗◒◗ ▷▪▢▱◠▦▪◓ ◚▴▽◠ ◳◠◚◠◒▱◠◓▵',\n",
       " '◆▱◧◀◠▱ ▬◫▫◗▢▴▦ ◕◓▾◀▾■ ▫◪◎◫▦◠▫ ◠▱▫▪▦◠ ◠▱◬▦◠▦ ◍◧▦▱◠◓◬▦ ◒◗◎◈◗▽▴ ▨◠◈◠◓ ◈▩▦▽◠ ◕▴▦▴▱◗▦◈◪ ▽◠▨▱◠◒◬▨ 649 ◎◗▱▽◧▦ ◫▦◞◠▦◬▦ ▷◠▽◠▫◬▦▪ ◈◂◐◓◨◈◠▦ ▴▫▨◫▱▴◈◫◐◫▦◗ ▫◠▷◎◫▦ ◪◈◗▽◧◓▵',\n",
       " \"◟▴◓◪▱ ◎◪◈◳◠ ◓◠▻◧◓◨▦◠ ◕◣◓▴■ ◅◫▦'◫▦ ◕◭▦◪▽◀◠▫◬◞◬▦◈◠ ◀◗◓ ◉◠◓◒◬◈◠ ◉◫◍▫◉◗▦◗▦ ◀◗◓◫ ◈◧◎▾▢▾▦ ◞◠▱◈◬◓◬◞◬▦◠ ▾◐◓◠◳▪▻ ◇▱◈▩▵\",\n",
       " '◙◫◠▦▦▴ ◌◪◫▦◞▫◪◗▦■ ◀◗▱◕◗ ◞◬▢◈▪◓◈◬▦▪▢ ◎▪?',\n",
       " \"○◞▪▱ ◀◪◳◠▢ ◄▴▫◂◈◫◞▫ ▼◪◎◠◠▫ 1930'▱◠◓◈◠ ▫◠◒◬▦◈◬▵\",\n",
       " '\"◩▫◪ ▽◠▦◈◠▦■ □◪▨◫▦ ◞◂▨◠▨▱◠◓◬▦◈◠▨◗ ◀◗◓ ◕◠▢◪▫▴ ◀◠◳◗◞◗▦◪ ◕◫▫▫◗◐◫▦◗▢◈◪ ◎◨▷◠▱◫◍ ◕◣◓◭◒▱◪◓◗ ▽◠◳◬▦▱◠◳◠▦ ◠▢ ◞◠▽▪◈◠ ◕◠▢◪▫▴ ◂▱◈▾◐▾▦▾ ◕◣◓◪◀◗▱◗◓◞◫▦◗▢ ◚▴ ◅◫▦▱◗▱▴◓◗▦ ◭▱▨▴▦◗▦ ▢◂◓ ◈◨◓▾◎◈◠▨◫ ◪▨◂▦◧◎◫▨ ◕◫◈◫◒◠▫▪▽▱◠ ◫▱◕◗▱◫ ◍◠◓▨▱▪ ◕◣◓▩◒▱◪◓◗▦◗ ◈◂◐◓◨ ◒◪▨◫▱◈◪ ▽◠▦◞▪▫◠▦ ▷◗◉◀◫◓ ▽◠▽◬▦ ◀◨▱◠◎◠▢◞▪▦◬▢▵ ◅◭▦▨◭ ◎▴◈◳◠■ ◅◫▦ ◢◧◎▩▦◫◞▫ □◠◓▫◗◞◫\\'▦◫▦ ◞◬▨▪ ▨◂▦▫◓◧▱◭ ◠▱▫◬▦◈◠◈◬◓■\" ◈◗◳▴ ◳◠▢◈▪▵',\n",
       " '◄◂◓◠▱▴◞\\'▴ ◕◇◓▴ \"◝◧▱◗◚◳◠ ◠◞▱◠ □◠◞◫◍◫▨ ◱▨◳◠▦◨◞▾\\'▦◠ ◪◓◗◒◫◎ ▷◠▨▨◬▦◈◠▦ ◚◠▢◕▴◉◎◪▽◪▼▴▨▵\"',\n",
       " \"□◂◎▻◪◧ ◢◨▢◪◳ ◢◂◓▴'▽◫ ◀▾ ▽◬▱ ◒◫◎◈◫◈◪▦ ◭◉ ▨◪▢ ▢◗◳◠◓◪▫ ▴▫▫◗■ ◠▦▼◠▨ ◞◂▦ ▢◗◳◠◓▴▫◗ ◫▽◫ ◕◪◉◎◪◈◗▵\",\n",
       " \"◢◬▢◬▱ ◆◪▱◕◫▫ ◠◳◓◬▼◠ □◠◞▼◂ ◝◇▱◕▴◞◫'▦◈▴ ◈◪ ◕◇▢▱◪▦◈◗▵\",\n",
       " \"○◎◠ ◕◪◓◉▴▨▫◪▦ ◚◪◓◎◪▨ ◫◞▫◪◈◫◐◗◎ ◎◪◞◠▸ ▯◧▫▫◫▦◕▷◠◎ ◄◂◎◪▦▫◨◎'◠ ◈▴◐◗▱■ ◲◒◉◗ □◠◓▫◗◞◫ ◭▽◪◞◗ ◂▱◠▦ ◚◪ ◂▱◎◠◳◠▦ ◞▴◉◎▴▦▱◪◓◗◎▴: ▮◗▢◪ ▷◫▢◎▴▫ ▴▫◎◪▨▫◪▦ ◕▾◓◨◓ ◈▾▽▾▽◧◓▾◎ ◚▴ ▷◫◉◀◫◓ ◠▽▪▨▱◠◎◠ ▫▴▷◈◫◈◫ ◚▴▽◠ ◞◗▽◠◞◫ ◉◬▨◠◓▪▦■ ▷▴▻◫▦◗▢◫▦ ◉◬▨◠◓◬▦◠ ◪▦ ◗◳◗ ◒▴▨◗▱◈◪ ▷◫▢◎▴▫ ◪▫▫◗◐◗▦▴ ◗▦◠▦◈▪◐▪◎ ◒▴◳▱◪◓◫ ▽◠▻◎◠◎◠ ◪▦◕▴▱ ◧▱◠◎◠▽◠▼◠◐▪▦◠ ◞◇▢ ◚▴◓◫◳◧◓▾◎▵\",\n",
       " '▤◧◞◪▦◞▫◪◗▦ ▷◠◀▴◓◗ ◳◠▱◠▦▱◠◈▪▵',\n",
       " \"▬▾◎◠ ◕◭▦▩ ◘▦◈◂▦◪▢◳◠'▦▪▦ ▮◨▱◠▥◪◞◗ ◠◈◠◞◬▦◈◠ ◎◪◳◈◠▦◠ ◕▴▱◪▦ ◈◪▻◓◪◎◈▴ ▴▦ ◠▢ 384 ▨◗◒◗ ▷◠▽◠▫▪▦▪ ▨◠▽◀▴▫▫◫▵\",\n",
       " '◖▴▷◫◓■ ▶▩◓▨◫▽▴ ◈◬◒▪▦◈◠▨◗ ▴▦ ◀◭◳▩▨ ▶▩◓▨ ▦▩◍◨◞▾▦◨ ◀◠◓▪▦◈▪◓◬◳◧◓▵',\n",
       " \"▶◓◨◎▻ ◝▴◳◠▢ ▮◠◓◠◳'◈◠▦ ◉◬▨◠◓▨▴▦ ◚▴◳◠ ▽◭▨◞◪▨ ◓◭▫◀◪▱◗ ▢◫▽◠◓◪▫◉◫▱◪◓▱◪ ◕▴◓◉▴▨▱◪◒▫◫◓◈◗◐◫ ◠◉▪▨ ◧▫▾◓▾◎▱◠◓◠ ◳◠ ◈◠ ◀◠◞◬▦ ▨◂▦◍◪◓◠▦◞▱◠◓▪▦◠ ▨◠▫▪▱◈◬◐◬ ▢◠◎◠▦ ◈▩▢▴▦▱◗ ◂▱◠◓◠▨ ◞◧◓▾▱◠◓◠ ▽◠▦◬▫ ◚◪◓◫▽◂◓▵\",\n",
       " \"◆◓◠▼▴■ ▶◪▨◞◠◞'▫◠ ◕◣▢ ◠▱▫▪▦◠ ◠▱▪▦◈▪▵\",\n",
       " '◄◠▷▨◪◎◪ ◀▴▱◕◪▱▴◓◗▦▴ ◕◇◓◪■ ◀◨ ◗▨◗ ◞◨◉ ◈◠ ◎◨▷▫◪◎▴▱ ◀◫◓ ◗◈◠◎ ▼▴▢◠◞▪▦◬ ◀▴◓◠◀◪◓◫▦◈◪ ◕◪▫◫◓◗▽◂◓▵',\n",
       " '◙◫◐▴◓ ◀◭▽◭▨ ◀◂▽ ◠▨◞▴◞▾◠◓▱◠◓ ◠◓◠◞◬▦◈◠ ◈▴◐◪◓▱◗ ▫◠◒▱◠◓▱◠ ◞▩◞▱◪▦◎◫◒ ▨◠◓▻◨▢▱◠◓■ ◀◫◓ ◀◭▽◭▼▩ ◒◠▻▨◠◞◬■ ▷◠▫▫◠ ◀◫◓ ◠▦◠▦◠◞ ◀◫▱▴ ◀▾▱◨▦◨▽◧◓ ◠◎◠ ▴▱◀◪▫▫▴ ◀▾ ◠▨◞◪◞▾◠◓▱◠◓ ▨◠◍◠▦▪▢◬ ◞◧◐◨▨▫◠▦ ▨◧◓◨◎◠▨ ◫◉◗▦ ▫◠◞◠◓▱◠▦◎◠◎◬◒▵',\n",
       " '◘◎▢◫◓▴▦ ◠▦▦▴▱▴◓ ◚◪ ◞◠◐◎◠▱ ◗▦▴▨▱▴◓ ◠◓◠◞◬▦◈◠ ◀◪▦▢▴◓▱◫▨ ▨▾◓◠◓◠▨■ ◳◪▦◫ ▻◂◎▻◠▦▪▦ ◞◠◐▱◠◳◠▼◠◐▪ ◎◨▷▫◪◒◪◎ ◣▢◕▩◓▱◭▨ ▷◗◞◞◗▦◫ ◪◐▱◪▦▼▴▱◫ ◚▴ ◠▦▱◠◳▪◒▱▪ ◀◗◓ ◒▴▨◗▱◈▴ ◕◇◞▫▴◓◗◓▨▴▦ ◞◭▫ ▻◧◎▻◠▱◠◎◠◳◬ ◚▴ ◀◨ ◗◒▱◪◎◗▦ ▫▩◎ ▢◧◓▱◨▨▱◠◓▪▦▪ ◕◭▦ ▪◒◬◐▪▦◬ ◉◬▨◠◓◎◠▨ ◗◞▫◪◈◫▨▵',\n",
       " \"◝◨ ▽▪▱ ▽◪◈◗▦▼◗◞◫ ◈◭▢◪▦▱▴▦◪▦ ◆▱◂◀◠▱ ▬◫▫◗▢▴▦ ◌◪◞▫◗◚◠▱◗ ◫◉◫▦ ◧▦ ◀◫▦▱◪◓▼▴ ◗▦◞◠▦ ▬▴▦▫◓◠▱ □◠◓▨'◠ ◠▨◬▦ ▴◈▴◓▴▨ ▷◪◎ ◡◠▦◪▫ ◡◠▼▨◞◂▦■ ▬◠◓◈◗ ◝ ◚◪ ▮▷◠▥▦ ◄▴▦◈◪◞ ◕◫◀◫ ◞◠▦◠▫◉▪▱◠◓▪▦ ▻◪◓◍◧◓◎◠▦◞▱◠◓◬▦▪ ◫▢▱◪◳▴▼◪▨ ▷◪◎ ◈◪ ◪▫▨◗▦▱◫◐◫▦ ◠◞▪▱ ▷▴◈▴◍◗ ◧▱◠▦ 2030'◠ ▨◠◈◠◓ ◠◒▪◓◬ ◳◂▨◞◨▱▱▾◐▾ ◞◂▦◠ ◪◓◈◗◓◎▴ ◎◫◞▽◧▦▾◳▱◠ ◫▱◕◗▱◗ ◍◠◓▨◬▦◈◠▱◬◐◬ ◠◓▫◬◓◠▼◠▨▵\"]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open('output.jsonl', mode='w') as writer:\n",
    "    for ds, sr in zip(trans, sente):\n",
    "        writer.write({\"dst\":ds, \"src\":sr})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
